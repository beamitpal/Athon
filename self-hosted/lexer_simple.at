// Athōn Lexer - Simple Working Version
// Self-hosted lexer demonstrating tokenization concepts

// Token kinds as integer constants
fn TK_FN() -> int { return 0; }
fn TK_LET() -> int { return 1; }
fn TK_IF() -> int { return 2; }
fn TK_ELSE() -> int { return 3; }
fn TK_WHILE() -> int { return 4; }
fn TK_RETURN() -> int { return 6; }
fn TK_LPAREN() -> int { return 20; }
fn TK_RPAREN() -> int { return 21; }
fn TK_LBRACE() -> int { return 22; }
fn TK_RBRACE() -> int { return 23; }
fn TK_SEMICOLON() -> int { return 26; }
fn TK_EQUALS() -> int { return 50; }
fn TK_PLUS() -> int { return 57; }
fn TK_MINUS() -> int { return 58; }
fn TK_STAR() -> int { return 59; }
fn TK_IDENTIFIER() -> int { return 70; }
fn TK_NUMBER() -> int { return 71; }
fn TK_UNKNOWN() -> int { return 99; }

// Character classification
fn is_whitespace(c: int) -> int {
    if c == 32 { return 1; }  // space
    if c == 9 { return 1; }   // tab
    if c == 10 { return 1; }  // newline
    if c == 13 { return 1; }  // carriage return
    return 0;
}

fn is_digit(c: int) -> int {
    return c >= 48 && c <= 57;  // '0' to '9'
}

fn is_alpha(c: int) -> int {
    if c >= 65 && c <= 90 { return 1; }   // 'A' to 'Z'
    if c >= 97 && c <= 122 { return 1; }  // 'a' to 'z'
    if c == 95 { return 1; }              // '_'
    return 0;
}

fn is_alnum(c: int) -> int {
    if is_alpha(c) == 1 { return 1; }
    if is_digit(c) == 1 { return 1; }
    return 0;
}

// Tokenization functions - return token kind
fn classify_char(c: int) -> int {
    // Whitespace
    if is_whitespace(c) == 1 {
        return 0;  // Skip
    }
    
    // Digits -> Number token
    if is_digit(c) == 1 {
        return TK_NUMBER();
    }
    
    // Letters -> Identifier token
    if is_alpha(c) == 1 {
        return TK_IDENTIFIER();
    }
    
    // Single-character tokens
    if c == 40 { return TK_LPAREN(); }    // '('
    if c == 41 { return TK_RPAREN(); }    // ')'
    if c == 123 { return TK_LBRACE(); }   // '{'
    if c == 125 { return TK_RBRACE(); }   // '}'
    if c == 59 { return TK_SEMICOLON(); } // ';'
    if c == 61 { return TK_EQUALS(); }    // '='
    if c == 43 { return TK_PLUS(); }      // '+'
    if c == 45 { return TK_MINUS(); }     // '-'
    if c == 42 { return TK_STAR(); }      // '*'
    
    return TK_UNKNOWN();
}

// Token kind to name
fn token_name(kind: int) {
    if kind == 0 { print("fn"); }
    if kind == 1 { print("let"); }
    if kind == 2 { print("if"); }
    if kind == 20 { print("lparen"); }
    if kind == 21 { print("rparen"); }
    if kind == 22 { print("lbrace"); }
    if kind == 23 { print("rbrace"); }
    if kind == 26 { print("semicolon"); }
    if kind == 50 { print("equals"); }
    if kind == 57 { print("plus"); }
    if kind == 58 { print("minus"); }
    if kind == 59 { print("star"); }
    if kind == 70 { print("identifier"); }
    if kind == 71 { print("number"); }
    if kind == 99 { print("unknown"); }
}

// Simulate lexing a simple expression: "x = 5 + 3;"
fn lex_expression() {
    print("Lexing: x = 5 + 3;");
    print("");
    
    // 'x' (120)
    let t1 = classify_char(120);
    print("  'x' -> ", "");
    token_name(t1);
    
    // ' ' (32) - whitespace
    let t2 = classify_char(32);
    if t2 == 0 {
        print("  ' ' -> (whitespace, skip)");
    }
    
    // '=' (61)
    let t3 = classify_char(61);
    print("  '=' -> ", "");
    token_name(t3);
    
    // ' ' (32) - whitespace
    let t4 = classify_char(32);
    if t4 == 0 {
        print("  ' ' -> (whitespace, skip)");
    }
    
    // '5' (53)
    let t5 = classify_char(53);
    print("  '5' -> ", "");
    token_name(t5);
    
    // ' ' (32) - whitespace
    let t6 = classify_char(32);
    if t6 == 0 {
        print("  ' ' -> (whitespace, skip)");
    }
    
    // '+' (43)
    let t7 = classify_char(43);
    print("  '+' -> ", "");
    token_name(t7);
    
    // ' ' (32) - whitespace
    let t8 = classify_char(32);
    if t8 == 0 {
        print("  ' ' -> (whitespace, skip)");
    }
    
    // '3' (51)
    let t9 = classify_char(51);
    print("  '3' -> ", "");
    token_name(t9);
    
    // ';' (59)
    let t10 = classify_char(59);
    print("  ';' -> ", "");
    token_name(t10);
}

// Lex function definition: "fn add(x) { }"
fn lex_function() {
    print("Lexing: fn add(x) { }");
    print("");
    
    // 'f' (102) -> identifier
    let t1 = classify_char(102);
    print("  'f' -> ", "");
    token_name(t1);
    
    // 'n' (110) -> identifier
    let t2 = classify_char(110);
    print("  'n' -> ", "");
    token_name(t2);
    
    // ' ' -> skip
    print("  ' ' -> (whitespace, skip)");
    
    // 'a' (97) -> identifier
    let t3 = classify_char(97);
    print("  'a' -> ", "");
    token_name(t3);
    
    // '(' (40) -> lparen
    let t4 = classify_char(40);
    print("  '(' -> ", "");
    token_name(t4);
    
    // 'x' (120) -> identifier
    let t5 = classify_char(120);
    print("  'x' -> ", "");
    token_name(t5);
    
    // ')' (41) -> rparen
    let t6 = classify_char(41);
    print("  ')' -> ", "");
    token_name(t6);
    
    // ' ' -> skip
    print("  ' ' -> (whitespace, skip)");
    
    // '{' (123) -> lbrace
    let t7 = classify_char(123);
    print("  '{' -> ", "");
    token_name(t7);
    
    // ' ' -> skip
    print("  ' ' -> (whitespace, skip)");
    
    // '}' (125) -> rbrace
    let t8 = classify_char(125);
    print("  '}' -> ", "");
    token_name(t8);
}

fn main() {
    print("=== Athōn Self-Hosted Lexer ===");
    print("");
    print("A lexer written in Athōn that tokenizes Athōn source code");
    print("");
    
    // Character classification tests
    print("Character Classification:");
    print("  is_whitespace(32): {}", is_whitespace(32));
    print("  is_digit(53):      {}", is_digit(53));
    print("  is_alpha(97):      {}", is_alpha(97));
    print("  is_alnum(53):      {}", is_alnum(53));
    print("");
    
    // Token classification tests
    print("Token Classification:");
    print("  '(' (40)  -> kind {}", classify_char(40));
    print("  ')' (41)  -> kind {}", classify_char(41));
    print("  '5' (53)  -> kind {}", classify_char(53));
    print("  'a' (97)  -> kind {}", classify_char(97));
    print("  '+' (43)  -> kind {}", classify_char(43));
    print("  '=' (61)  -> kind {}", classify_char(61));
    print("");
    
    // Lex example expressions
    lex_expression();
    print("");
    lex_function();
    print("");
    
    print("=== Lexer Demo Complete ===");
    print("");
    print("This demonstrates the core concepts of lexical analysis:");
    print("  1. Character classification (whitespace, digit, alpha)");
    print("  2. Token recognition (keywords, operators, literals)");
    print("  3. Tokenization of source code into token stream");
}
